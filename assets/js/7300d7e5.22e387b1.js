"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[7082],{1729:e=>{e.exports=JSON.parse('{"blogPosts":[{"id":"Surpassing 1 Million Downloads - A Retrospective and a Look into the Future","metadata":{"permalink":"/FLAML/blog/2023/05/07/1M-milestone","source":"@site/blog/2023-05-07-1M-milestone/index.mdx","title":"Surpassing 1 Million Downloads - A Retrospective and a Look into the Future","description":"TL;DR:","date":"2023-05-07T00:00:00.000Z","formattedDate":"May 7, 2023","tags":[{"label":"LLM","permalink":"/FLAML/blog/tags/llm"},{"label":"LLMOps","permalink":"/FLAML/blog/tags/llm-ops"},{"label":"FLAMLv2","permalink":"/FLAML/blog/tags/flam-lv-2"}],"readingTime":3.615,"truncated":false,"authors":[{"name":"Qingyun Wu","title":"Assistant Professor at the Pennsylvania State University","url":"https://qingyun-wu.github.io/","imageURL":"https://github.com/qingyun-wu.png","key":"qingyunwu"}],"nextItem":{"title":"Does Model and Inference Parameter Matter in LLM Applications? - A Case Study for MATH","permalink":"/FLAML/blog/2023/04/21/LLM-tuning-math"}},"content":"**TL;DR:**\\n* **Celebrating FLAML\'s milestone: 1 million downloads**\\n* **Introducing Large Language Model (LLM) support in the upcoming FLAML v2**\\n\\n\\nThis week, FLAML has reached a significant milestone: 1 million downloads. Originating as an intern research project within Microsoft Research, FLAML has grown into an open-source library used widely across the industry and supported by an active community.\\nAs we celebrate this milestone, we want to recognize the passionate contributors and users who have played an essential role in molding FLAML into the flourishing project it is today. Our heartfelt gratitude goes out to each of you for your unwavering support, constructive feedback, and innovative contributions that have driven FLAML to new heights.\\nA big shoutout to our industrial collaborators from Azure Core, Azure Machine Learning, Azure Synapse Analytics, Microsoft 365, ML.NET, Vowpal Wabbit, Anyscale, Databricks, and Wise; and academic collaborators from MIT, Penn State University, Stevens Institute of Technology, Tel Aviv University, Texas A & M University, University of Manchester, University of Washington, and The Chinese University of Hong Kong etc.\\n\\nWe\'d also like to take the opportunity to reflect on FLAML\'s past achievements and its future roadmap, with a particular focus on large language models (LLM) and LLMOps.\\n\\n## FLAML\'s Journey: Past Achievements and Milestones\\n\\n### Bring AutoML to One\'s Fingertips\\nFLAML offers an off-the-shelf AutoML solution that enables users to quickly discover high-quality models or configurations for common ML/AI tasks. By automatically selecting models and hyperparameters for training or inference, FLAML saves users time and effort. FLAML has significantly reduced development time for developers and data scientists alike, while also providing a convenient way to integrate new algorithms into the pipeline, enabling easy extensions and large-scale parallel tuning. These features make FLAML a valuable tool in R&D efforts for many enterprise users.\\nFLAML is capable of handling a variety of common ML tasks, such as [classification](https://microsoft.github.io/FLAML/docs/Examples/AutoML-Classification), [regression](https://microsoft.github.io/FLAML/docs/Examples/AutoML-Regression), [time series forecasting](https://microsoft.github.io/FLAML/docs/Examples/AutoML-Time%20series%20forecast), [NLP tasks](https://microsoft.github.io/FLAML/docs/Examples/AutoML-Rank), and [generative tasks](https://microsoft.github.io/FLAML/docs/Use-Cases/Auto-Generation), providing a comprehensive solution for various applications.\\n\\n### Speed and Efficiency: The FLAML Advantage\\nWhat sets FLAML apart from other AutoML libraries is its exceptional efficiency, thanks to the economical and efficient hyperparameter optimization and model selection methods developed in our [research](https://microsoft.github.io/FLAML/docs/Research). FLAML is also capable of handling large search spaces with heterogeneous evaluation costs, complex constraints, guidance, and early stopping. The [zero-shot AutoML](https://microsoft.github.io/FLAML/docs/Use-Cases/Zero-Shot-AutoML) option further reduces the cost of AutoML, making FLAML an even more attractive solution for a wide range of applications with low resources.\\n\\n### Easy Customization and Extensibility\\nFLAML is designed for easy extensibility and customization, allowing users to add custom learners, metrics, search space, etc. For example, the support of hierarchical search spaces allows one to first choose an ML learner and then sampling from the hyperparameter space specific to that learner. The level of customization ranges from minimal (providing only training data and task type as input) to full (tuning a user-defined function). This flexibility and support for easy customization have led to FLAML\'s adoption in various domains, including security, finance, marketing, engineering, supply chain, insurance, and healthcare, delivering highly accurate results.\\n\\n## Embracing Large Language Models in FLAML v2\\nAs large language models continue to reshape the AI ecosystem, FLAML is poised to adapt and grow alongside these advancements. Recognizing the importance of large language models, we have recently incorporated an autogen package into FLAML, and are committed to focusing our collective efforts on addressing the unique challenges that arise in LLMOps (Large Language Model Operations).\\n\\nIn its current iteration, FLAML offers support for model selection and inference parameter tuning for large language models. We are actively working on the development of new features, such as LLM selection, inference hyperparameter tuning for LLM, and agent-based LLM operations, to further expand FLAML\'s capabilities.\\n\\nWe are eagerly preparing for the launch of FLAML v2, where we will place special emphasis on incorporating and enhancing features specifically tailored for large language models (LLMs), further expanding FLAML\'s capabilities.\\nWe invite contributions from anyone interested in this topic and look forward to collaborating with the community as we shape the future of FLAML and LLMOps together.\\n\\n## For Further Reading\\n\\n* [Documentation about `flaml.autogen`](/docs/Use-Cases/Auto-Generation)\\n* [Code Example: Tune chatGPT for Math Problem Solving with FLAML](https://github.com/microsoft/FLAML/blob/main/notebook/autogen_chatgpt_gpt4.ipynb)\\n\\n*Do you have any experience to share about LLM applications? Do you like to see more support or research of LLMOps? Please join our [Discord](https://discord.gg/Cppx2vSPVP) server for discussion.*"},{"id":"Does Model and Inference Parameter Matter in LLM Applications? - A Case Study for MATH","metadata":{"permalink":"/FLAML/blog/2023/04/21/LLM-tuning-math","source":"@site/blog/2023-04-21-LLM-tuning-math/index.mdx","title":"Does Model and Inference Parameter Matter in LLM Applications? - A Case Study for MATH","description":"level 2 algebra","date":"2023-04-21T00:00:00.000Z","formattedDate":"April 21, 2023","tags":[{"label":"LLM","permalink":"/FLAML/blog/tags/llm"},{"label":"GPT","permalink":"/FLAML/blog/tags/gpt"},{"label":"research","permalink":"/FLAML/blog/tags/research"}],"readingTime":4.815,"truncated":false,"authors":[{"name":"Chi Wang","title":"Principal Researcher at Microsoft Research","url":"https://www.linkedin.com/in/chi-wang-49b15b16/","imageURL":"https://github.com/sonichi.png","key":"sonichi"}],"prevItem":{"title":"Surpassing 1 Million Downloads - A Retrospective and a Look into the Future","permalink":"/FLAML/blog/2023/05/07/1M-milestone"}},"content":"![level 2 algebra](img/level2algebra.png)\\n\\n**TL;DR:**\\n* **A case study using the MATH benchmark shows that model selection and inference parameters do matter in Large Language Model (LLM) applications.**\\n* **The tuned gpt-3.5-turbo model vastly outperformed untuned gpt-4 in accuracy for easier problems, while gpt-4 was a better choice for the most difficult problems.**\\n* **FLAML can help with model selection, parameter tuning, and cost-saving in LLM applications.**\\n\\n\\nLarge language models (LLMs) are powerful tools that can generate natural language texts for various applications, such as chatbots, summarization, translation, and more. GPT-4 is currently the state of the art LLM in the world. Is model selection irrelevant? What about inference parameters?\\n\\nIn this blog post, we will explore how model and inference parameter matter in LLM applications, using a case study for [MATH](https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/be83ab3ecd0db773eb2dc1b0a17836a1-Abstract-round2.html), a benchmark for evaluating LLMs on advanced mathematical problem solving. MATH consists of 12K math competition problems from AMC-10, AMC-12 and AIME. Each problem is accompanied by a step-by-step solution.\\n\\nWe will use the new subpackage [`flaml.autogen`](docs/Use-Cases/Auto-Generation) to automatically find the best model and inference parameter for LLMs on a given task and dataset given an inference budget, using a novel low-cost search & pruning strategy. FLAML currently supports all the LLMs from OpenAI, such as GPT-3.5 and GPT-4.\\n\\nWe will use FLAML to perform model selection and inference parameter tuning. Then we compare the performance and inference cost on solving algebra problems with the untuned gpt-4. We will also analyze how different difficulty levels affect the results.\\n\\n## Experiment Setup\\n\\nWe use FLAML to select between the following models with a target inference budget $0.02 per instance:\\n- gpt-3.5-turbo, a relatively cheap model that powers the popular ChatGPT app\\n- gpt-4, the state of the art LLM that costs more than 100 times of gpt-3.5-turbo\\n\\nWe adapt the models using 20 examples in the train set, using the problem statement as the input and generating the solution as the output. We use the following inference parameters:\\n\\n- temperature: The parameter that controls the randomness of the output text. A higher temperature means more diversity but less coherence. We search for the optimal temperature in the range of [0, 1].\\n- top_p: The parameter that controls the probability mass of the output tokens. Only tokens with a cumulative probability less than or equal to top-p are considered. A lower top-p means more diversity but less coherence. We search for the optimal top-p in the range of [0, 1].\\n- max_tokens: The maximum number of tokens that can be generated for each output. We search for the optimal max length in the range of [50, 1000].\\n- n: The number of responses to generate. We search for the optimal n in the range of [1, 100].\\n- prompt: We use the template: \\"{problem} Solve the problem carefully. Simplify your answer as much as possible. Put the final answer in \\\\\\\\boxed{{}}.\\" where {problem} will be replaced by the math problem instance.\\n\\nIn this experiment, when n > 1, we find the answer with highest votes among all the responses and then select it as the final answer to compare with the ground truth. For example, if n = 5 and 3 of the responses contain a final answer 301 while 2 of the responses contain a final answer 159, we choose 301 as the final answer. This can help with resolving potential errors due to randomness. We use the average accuracy and average inference cost as the metric to evaluate the performance over a dataset. The inference cost of a particular instance is measured by the price per 1K tokens and the number of tokens consumed.\\n\\n## Experiment Results\\n\\nThe first figure in this blog post shows the average accuracy and average inference cost of each configuration on the level 2 Algebra test set.\\n\\nSurprisingly, the tuned gpt-3.5-turbo model is selected as a better model and it vastly outperforms untuned gpt-4 in accuracy (92% vs. 70%) with equal or 2.5 times higher inference budget.\\nThe same observation can be obtained on the level 3 Algebra test set.\\n\\n![level 3 algebra](img/level3algebra.png)\\n\\nHowever, the selected model changes on level 4 Algebra.\\n\\n![level 4 algebra](img/level4algebra.png)\\n\\nThis time gpt-4 is selected as the best model. The tuned gpt-4 achieves much higher accuracy (56% vs. 44%) and lower cost than the untuned gpt-4.\\nOn level 5 the result is similar.\\n\\n![level 5 algebra](img/level5algebra.png)\\n\\nWe can see that FLAML has found different optimal model and inference parameters for each subset of a particular level, which shows that these parameters matter in cost-sensitive LLM applications and need to be carefully tuned or adapted.\\n\\nAn example notebook to run these experiments can be found at: https://github.com/microsoft/FLAML/blob/v1.2.1/notebook/autogen_chatgpt.ipynb\\n\\n## Analysis and Discussion\\n\\nWhile gpt-3.5-turbo demonstrates competitive accuracy with voted answers in relatively easy algebra problems under the same inference budget, gpt-4 is a better choice for the most difficult problems. In general, through parameter tuning and model selection, we can identify the opportunity to save the expensive model for more challenging tasks, and improve the overall effectiveness of a budget-constrained system.\\n\\nThere are many other alternative ways of solving math problems, which we have not covered in this blog post. When there are choices beyond the inference parameters, they can be generally tuned via [`flaml.tune`](docs/Use-Cases/Tune-User-Defined-Function).\\n\\nThe need for model selection, parameter tuning and cost saving is not specific to the math problems. The [Auto-GPT](https://github.com/Significant-Gravitas/Auto-GPT) project is an example where high cost can easily prevent a generic complex task to be accomplished as it needs many LLM inference calls.\\n\\n## For Further Reading\\n\\n* [Research paper about the tuning technique](https://arxiv.org/abs/2303.04673)\\n* [Documentation about `flaml.autogen`](/docs/Use-Cases/Auto-Generation)\\n\\n*Do you have any experience to share about LLM applications? Do you like to see more support or research of LLM optimization or automation? Please join our [Discord](https://discord.gg/Cppx2vSPVP) server for discussion.*"}]}')}}]);